{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = Request('https://paperswithcode.com/search?q=nlp', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "soup = BeautifulSoup(webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Titles\n",
    "titles = []\n",
    "\n",
    "x = soup.find_all('h1')\n",
    "for node in x:\n",
    "    titles.append(''.join(node.find_all(text=True)))\n",
    "# titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper links\n",
    "papers = []\n",
    "x = soup.find_all('div', attrs={'class':'entity'})\n",
    "for i in x:\n",
    "    papers.append(i.find('a', attrs={'class':'badge badge-light'})['href'])\n",
    "\n",
    "    \n",
    "base_url = 'https://paperwithcode.com'\n",
    "papers_links = []\n",
    "for i in range(len(papers)):\n",
    "    papers_links.append('https://paperwithcode.com{}'.format(papers[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"badge badge-light\" href=\"/paper/automating-biomedical-data-science-through\">\n",
       "<ion-icon name=\"document\"></ion-icon> Paper\n",
       "                    </a>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code links\n",
    "\n",
    "code = []\n",
    "\n",
    "x = soup.find_all('div', attrs={'class':'entity'})\n",
    "for i in x:\n",
    "    code.append(i.find('a', attrs={'class': 'badge badge-dark'})['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_code = []\n",
    "\n",
    "for i in range(len(code)):\n",
    "    github_code.append('https://paperwithcode.com{}'.format(code[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models.',\n",
       " 'Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data.',\n",
       " 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks.',\n",
       " 'We present OpenSeq2Seq - a TensorFlow-based toolkit for training sequence-to-sequence models that features distributed and mixed-precision training.',\n",
       " 'Deep Neural Networks (DNN) have been widely employed in industry to address various Natural Language Processing (NLP) tasks.',\n",
       " 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration.',\n",
       " 'Community Question-Answering websites, such as StackOverflow and Quora, expect users to follow specific guidelines in order to maintain content quality.',\n",
       " 'On GLUE, we attain within 0. 4% of the performance of full fine-tuning, adding only 3. 6% parameters per task.',\n",
       " '',\n",
       " 'In the first task, we show that simple models can predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# description\n",
    "\n",
    "desc = []\n",
    "'item-strip-abstract'\n",
    "x = soup.find_all('p', attrs={'class':'item-strip-abstract'})\n",
    "for i in range(len(x)):\n",
    "    desc.append(x[i].text)\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a keyword: science\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import quote_plus\n",
    "\n",
    "user = input('Enter a keyword:')\n",
    "qstr = quote_plus(user)\n",
    "print(qstr)\n",
    "\n",
    "req = Request('https://paperswithcode.com/search?q={0}'.format(qstr), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "soup = BeautifulSoup(webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dfadsf+dsafds'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bita3e2b9284fe241009277b40c2dcf841c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
